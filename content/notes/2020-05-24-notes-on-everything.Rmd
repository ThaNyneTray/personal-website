---
title: Notes On Everything
author: Desmond Tuiyot
date: '2020-05-24'
slug: notes-on-everything
categories:
  - Statistics
  - Personal Notes
tags:
  - Data Analysis
  - Notes
  - Random
bibliography: ref.bib
# csl: style.csl 
---

```{r include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```
## Disclaimer:  
These are my personal notes on things I learn. They are neither extensive nor comprehensive and will probably not be useful to you. There may be a lot of mistakes and errors because this space represents my attempts at understanding the things I learn.

#### Jump to:
* [Javascript](#javascript)
* [Front End Libraries](#frontend)
  * [Bootstrap](#bootstrap)
  * [jQuery](#jquery)
  * [localStorage](#local_storage)
* [Statistics](#statistics)
* [Probability](#probability)
* [Machine Learning](#machine_learning)
* [Algorithms](#algos)
* [Data Structures](data_structures)
* [Python](#python)
* [RShiny](#rshiny)

## Javascript {#javacsript}
### Basic Javascript  
#### Variables
* Basic data types : `undefined`, `null`, `object`, `symbol`, `string`, `boolean`, `number`
* Declaring : `var myVarname;`
* The `undefined` data type - this is the initial data type of variables that have been declared and not initialized. Adding or concatenating to an `undefined` variable leads to `NaN`/`"undefined"`.
* Best practice -> use camelCase for variable names  

#### Numbers
* Arithmetic operators - the usual plus operators `i++` & `i--`, `i+=someValue` (augmented addition), for each basic operator.
* `Remainder` operator `%` is similar to the `modulus` operator but does not work with negative numbers

#### Strings
* Escape sequences - per usual
* Concatenating Strings - use `+`, `+=`
* Length of string - use `.length` property on the target string
* String indexing - `someString[0]`
* String are immutable
* Indexing into last element - can't use `someString[-1]` like in Python. How unfortunate

#### Arrays
* Declaration = `var myArray = [el1, el2, ...]`. Just like any other list, it can contain different data types, including other lists
* Mutable, indexing is same as Python
* Appending to array - `someArray.push(someVal)`
* Removing from end of array - `someArray.pop()` - this also returns the popped element
* To remove from beginning of array -  `someArray.shift()`  - this is interesting.
* To add to beginning of array - `someArray.unshift(someVal)` - also interesting

#### Functions
* Declaration
  ```
  function functionName(){
    console.log("This is a function")
  }
  ```
* Parameter - pass by value or by reference?
* Scope
  * Global scope - variables declared outside functions have Global scope. Variables declared inside functions without using the `var` keyword also get Global scope
  * Local scope - variables declared within functions and function parameters
  * Local scope is prioritized over Global scope
* `return` keyword to return a value. Functions with no return value have `undefined` return value

#### Booleans, Comparisons, If statements
* `true`,`false`
* `if` conditions
  ```
  if (condition){
    // statement
  }else if (condition){
    // other statement
  }else {
    // other other statement
  }
  ```
* `==` - equality operator. Javascript can compare 2 different data types - it coerces one type to another then compares them.  
  `!=` - inequality operator
* `===` - strict equality operator - javascript does not convert the variables involved  
  `!==` - strict inequality operator
* `>`, `<`, `<=`, `>=` - these will also convert both variables to a common data type before comparison. No `strict` alternatives?
* `condition && condition` - Logical `and` operator
* `condition || condition` - Logical `or` operator

#### Switch Statements
* Switch statements are evaluated using strict equality. Replace long `if-else if` chains with `switch` whenever appropriate.
  ```
  switch(variable/value){
    case option1:
      // do sth
      break;
    case option2:
      // do sth
      break;
    case option3:
      // do sth
      break;
      ...
      ...
    default:
      // do default thing
      break
  }
  ```
* You can omit the `break` statement to tie multiple cases to one output
  ```
  switch(variable){
    case 1:
    case 2:
    case 3:
      // do sth
      break;
    case 4:
      // do sth
      break;
    ...
    ...
    default:
      // do sth
      break;
  }
  ```
#### Return values
* Just like Python (and probably many other languages) you can use a comparison statement as a return value -- `return a < b`

#### Javascript Objects
* Are they just dictionaries? Also, non-string properties are automatically typecasted to string.
  ```
  var me {
    "name":"Desmond Tuiyot",
    "age":"Over 20",
    "passions":["Dance", "Software Dev", "Data Science"]
  }
  ```
* Accessing properties of an object - Either dot notation, `object.prop1`, or square brackets, `object["prop"]`. With the bracket notation, you can store property name in a variable and use it to access the property.
* Updating and adding new properties - same syntax
* Deleting properties - `delete someObject.someProperty;`
* Check for existence of properties - `someObject.hasOwnProperty(propName);`
* Javascript object is a flexible data structure - allows for arbitrary combinations of strings, numbers, booleans, arrays, functions, and objects.
* JSON - Javascript Object Notation - makes sense


## Front End Libraries {#frontend}
### Bootstrap {#bootstrap}
* Boostrap figures out the width of your screen and respond by resizing it - hence responsive design. 
* Wrap everything in `<div class="fluid-container"> ... </div>`- bootstrap fluid containers?
* Making images responsive - `<img class="img-responsive" src="...">`
* Centering text  - add `class="text-center"`
* Button - `<button class="btn btn-...">ButtonText</>`
  + You can make the button stretch out to full horizontal page width by adding `btn-block` to the class
  + Bootstrap button color rainbow - `btn-primary`,`btn-info` (highlight optional action?), `btn-danger`
* Bootstrap uses a 12-column grid system. Makes it easy to arrange elements and specify relative width of them on the grid.
  + Most bootstrap classes can be applied to `div` elements
  + Bootstrap has different column width properties that can be applied depending on screen width
  + `row` class + `col-..-..` family of classes to arrange elements.
* `text-primary` - color of text
* You can use span to target inline elements, and even style several elements differently within the same line.
* Adding **Font Awesome** icons to buttons - they are either webfonts or vector graphics and are treated and can be styled just like text. 
  + You can add it like this `<i class="fas fa-info-circle"></i>`. You can also use `span` for this.
* Responsively style radio buttons and checkboxes
  + Same idea as before - wrap the radio buttons with `div class="row"` then each individual radion button with `div class="col-..-.."`
* Styling text inputs with `class="form-control"` makes them take up 100% width. 
* Creating visual sense of depth - class `well`
* Remember `ids` have to be unique for each element. 

### jQuery {#jquery}
* `document ready` function ensures that all the html contents are loaded before the javascript function is run
  ```
  $(document).ready(function(){
    // some statements
  });
  ```
* jQuery functions start with a `$` sign. It selects sth with a selector and does whatever to that element 
  * Targeting HTML elements using selectors - `$("button").addClass("animated bounce")`
  * Targeting by classes -> `$(".className").doWhateverHere("...")`
  * Targeting by id -> `$("#idName").doSumn("...")`
* Adding/removing classes
  * `.addClass()` does what it says it does to elements
  * `.removeClass()` also does what it says
* Changing css of selected element -`.css("attribute", "value")`
* Disable an element using jQuery - `.prop("disabled", true)` to adjust properties of elements
* Changing text - 
  * use `.html()` to set html tags and text inside an element. Previous content will be replaced.
  * user `.text()` to add just text
* Removing an element - `.remove()`
* Moving elements - `.appendTo()` - allows you to select elements and append them to another element
* Clone an element using `.clone()`
* Also function chaining is a thing - `$("#target").clone().appendTo("#destination")`
* Selecting parent/children - `.parent()`, `.children()`
* Targeting nth child of a selected element
  * `$("target:nth-child(n)")`, where n is the nth child. `target` can't be an id?
* Targeting odd and even elements - `$("target:odd/even")...`

### localStorage {#local_storage}
* Local Storage allows apps and sites to store data in the browser with no expiration date. So data is persistent across sessions, which is great.
* The main problem with HTTP as a transport layer is that it's `stateless` - when you use a web app and close it, its state will be reset next time you open it
* Normally you would store data pertaining to your interface server side, but what if you don't want to force your user to log in? 
* **Cookie** - text file stored in my pc and connected to the domain that my website runs on. I can store data pertaining to my domain
  * They add to the load of every document in my domain
  * Only allow for upto 4kb of storage
  * It's been used to spy on people before - so security conscious people turn it off and websites have to ask each time they wanna use it
* Five methods
  * `setItem` - `window.localStorage.setItem(key, value)`
    * `localStorage` can only store strings. To store arrays or objects, we convert them to strings
    using `JSONStringify(array?/object)` before passing them in.
  * `getItem` - `window.localStorage.getItem(key, value)`
    * if we had stored an object, we can use `JSON.parse(object)` to convert it from string
  * `removeItem` - `window.localStorage.removeItem(key)`
  * `clear` - `window.localStorage.clear()`
  * `key` - pass an integer to retrieve nth key - `window.localStorage.key(idx)`
* Checking for browser support
  ```
  if (typeof(Storage) !== "undefined"){
    // code for localStorage 
  }
  else {
    // no web storage support? Or sessionStorage??
  }
  ```
  
## Statistics {#statistics}

## Probability {#probability}

## Machine Learning {#machine_learning}

## Algorithms {#algos}
### Dynamic Programming - Patterns  
1. Minimum/Maximum path to reach a target  
These are straightforward optimization questions. You have a target to reach and at each step, you have some number of possible paths to take. The approach:
  + The subproblem is the minimum/maximum path to step `i`, where `0<=i<=n`
  + Choose the minimum/maximum path amongst all possible paths to reach the current state. Then add the value for the current state. 
  + Generate optimal solutions for all values leading up to and including the target, then return the value for the target. 
For each problem, I have to first think about the base cases, which I use to build the solution once I start iterating.  
The subproblem in this family of problems seem to be primarily prefixes - At subproblem `DP(i)`, I'm assuming that all the subproblems `DP(i)` depends on are already solved. 

2. Distinct Ways
* **Problem Statement:** Find the number of distinct ways to get to a target.
* **Approach:** Add up all the distinct ways to get to a state dp[i]
  ```python
  routes[i] = routes[i-1] + route[i-2] ... routes[i-k]
  ```
* Generate sum for all values leading up to the target and return the value for the target
  ```python
  for i in range(1, target+1):
    for j in range(len(ways)):
      if ways[j] < i: # this could be the number of steps you can take at a time 
        dp[i] += dp[i-ways[j]] 
  return dp[target]
  ```
* The `ways` array would be a list of routes to the current state. 
  + In the `coin change` problem, it is the number of coin denominations available to you. 
  + In the `climbing stairs` problem, it is the number of steps you can take at a time. 
* `dp[i-ways[j]]` represents the number of distinct ways to get to the spot that is ways[j] places behind the current state `dp[i]`.
  + `Coin change` problem - if my current state is the amount `amount = dp[i] = 20`, and one of the acceptable coin denominations, `ways[j] = 5`, then `dp[i-ways[j]]` is the number of distinct ways to get to state where `amount = 15`, which is already solved since we are using a bottoms up approach.
* Importantly, I shouldn't add anything to the sum `dp[i] += dp[i-ways[j]]`. If I know there's one way to get to `0`, and one way to get to `1`, then I know that there's 2 ways to get to `2`. 

3. Merging Intervals
* **Problem Statement:** Given a set of numbers find an optimal solution for a problem considering the current number and the optimal result from its left and right sides.
* **Approach:** Find the optimal solution for the interval `i,j` and return the best possible answer. Do this for each interval `i, j`
  ```python
  dp[i][j] = dp[i][k] + result[k] + dp[k+1][j]
  ```
* Find the optimal solution from left and right sides and add a solution for the current position
  ```python
  for l in range(1, n):
    for i in range(n-l):
      j = i + l
      for k in range(i, j):
        dp[i][j] = max(dp[i][j], dp[i][k]+result[k]+dp[k+1][j])
  return dp[0][n-1]
  ```

### Greedy Algorithms
* **The Greedy Paradigm:** Construct a solution iteratively by making `myopic`/`locally optimal` decisions, and hope that everything works out. 

#### Themes of the Greedy Paradigm/Contrast with Divide & Conquer    
* It's easy to propose multiple greedy algorithms for many problems, as opposed to D&C where decomposing a problem is harder.
* Easier running time analysis. For D&C we had to work much harder, and develop tools like the Master Method in order to establish running time.
  * Many greedy algorithms often come down to sorting your input and then performing some linear computation. The running time would be O(n*lgn)
* Hard to establish correctness of a greedy algorithm. Contrast D&C for which a straightforward inductive correctness proof can be done easily?
**DANGER:** Many greedy algorithms are NOT correct, even if my intuition says otherwise.

#### Proofs Of Correctness
How would you do it? -- Proving correctness of greedy algorithms is more art than science. However, there are recurring themes to it, and we will see what those are.

* **Method 1: ** `Induction` - Greedy algorithms make a number of irrevocable decisions, and so the induction will be on the decisions made by the algorithm 
  * AKA `greedy stays ahead` - you prove that the algorithm makes the corrrect decision iteration by iteration. 
  * We induct over the number of iterations?
  
* **Method 2: ** `Exchange Argument` - it has several flavors
  * `Proof by contradiction: `Assume that the greedy algorithm is incorrect. Take an "optimal" solution then exchange 2 elements of that optimal solution to get a better solution, thereby contradicting the assumption that you started with an optimal solution. I'm assuming you do the exchange based on the greedy algorithm
  * Gradually exchange an optimal solution into the one output by a greedy algorithm without making the algorithm worse. That would show that the algorithm is in fact correct. Formally, that's done by inducting on the number of exchanges required to transfer an optimal solution into mine
  
* **Method 3: **Whatever works.

#### Problem 1: A Scheduling Problem
**Problem Definition**

* The problem of scheduling involves how to schedule jobs on shared resources in order to accomplish some objective
* `Setup:`
  * One shared resource (i.e. a processor). 
  * Many jobs to do (i.e. processes).
* `Question:` In what order should we sequence these jobs?
* `Parameters:` Assume each job j has a:
  * **weight w~j~:** we can think of this as quantifying a job's priority. Higher priority needs to be processed first.
  * **length l~j~:** how long it takes for the job to be processed 
* `Output:` A sequence of these jobs in some particular order
* So given all the above, What are we trying to optimize? In order to define our `objective function` we need to define `completion time`.
  * `Completion time:` The completion time C~j~ for a job j is the sum of the job lengths of all jobs upto and including job j. It is the amount of time taken until j is completely processed. 
* `Objective function:` minimize the weighted sum of completion times. $min\sum_{j=1}^{n}C_j \cdot w_j$   

**A Greedy Algorithm**  

* We develop a greedy algorithm for solving the scheduling problem introduced above. However, I wanna pay attention to the process by which the greedy algorithm is developed.
* The process in summary is this:
  * We look at a special case of the problem, where it's fairly intuitive what the optimal solution should be
  * Looking at these special cases will then motivate a couple of greedy algorithms
  * We then narrow it down to one greedy algorithm, which we then prove will always be correct
* `Intuition For Algorithm:` Assume that a greedy algorithm for this problem does exist. How would we come up with that algorithm?
* **First**, focus on a couple of special cases where it's relatively clear how to proceed. In the scheduling problem, we have 2 cases:
  * With equal length jobs, should we schedule smaller or larger weight jobs first? -- **larger weights first**
  * With equal weight jobs, should we schedule shorter or longer length jobs first? -- **lower length first**. Scheduling a job at a certain position forces all other jobs to wait for that job to complete. So all else equal, you minimize the impact a particular assignment has on the rest of the jobs. That is, schedule a shorter job first, so the rest of them don't have to wait as long.
* **Second**, move beyond special cases which we understand well, to more general cases, which we don't understand.
  * If we have two jobs such that both of our rules of thumb give the same advice, we're good. That is, suppose job 1 has $w_1=4$, $C_1=3$ and job 2 has $w_1=2$, $C_1=4$, then job 1 should go first. Larger weight, lower length jobs go first.
  * However, if we have jobs that give conflicting advice, what do we do?
* `Resolving Conflicting Advice`
  * We could think about aggregating the 2 parameters, weight and length into one `score` such that if we calculate that score and sort, we could find an optimal schedule.
  * How do we aggregate the parameters? The formula to do this has to abide by our rules of thumb. That is, holding weight constant, if we feed in higher lengths, we should get a lower score. OTOH, holding length constant, if we feed in higher weights, we should get higher scores. 
    * **Formula 1:** GreedyDiff $w_j - l_j$
    * **Formula 2:** GreedyRatio $\frac{w_j}{l_j}$ 
* `Breaking a Greedy Algorithm` - because these greedy algorithms don't do the same thing, at most one of them is correct **(is this always the case?)**. 
  * This is something I will likely encounter in designing greedy algorithms. We have proposed two greedy algorithms here, so we want to rule out one of them. We can do this by providing an example for which one of them will not always work.
  * Come up with an example where the two algorithms provide different schedules, with different objective functions. The one that fares worse, then, is not always optimal. 
  * Example: job 1 $l_1=5, w_1=3$ and job 2 $l_2=2, w_2=1$. GreedyRatio algorithm prioritizes job1 while GreedyDiff prioritizes job2 first. Turns out GreedyRatio works better in this case, and is in fact correct in the general case.
  * Keep in mind, though, that just because we've eliminated GreedyDiff, doesn't automatically mean that GreedyRatio is correct. In fact, it is entirely possible to come up with multiple incorrect greedy algorithms for a specific problem.

**Proof of Correctness**

* `Claim:` GreedyRatio Algorithm is always correct - i.e., ordering jobs in decreasing order of ratio $w_j/l_j$ is always correct
* `Proof:` By an **exchange argument**. We make a simplifying assumption that there are no ties.
* `Proof Plan:`
  * Fix an arbitrary input of n jobs. Why? Because we are trying to prove that this algorithm is always correct. Therefore, we try to prove it for an arbitrary instance of the problem.
  * Proceed by contradiction. In general, we assume that our hypothesis is false, then proceed to derive something that is obviously false.
  * In the case of the scheduling problem, we assume that our greedy algorithm does not in fact produce the optimal scheduling.
  * Let $\sigma=$greedy schedule and $\sigma^*$=optimal schedule, with $\sigma^*$ being better
  * The plan is to produce an even better scheduling than $\sigma^*$, thereby contradicting our assumption that $\sigma^*$ was optimal.
* `Assume:` All $\frac{w_j}{l_j}$ are distinct. This is with loss of generality and we have a separate proof for cases where there are ties. 
* `Assume:` We rename jobs such that $\frac{w_1}{l_1}>\frac{w_2}{l_2}>\frac{w_3}{l_3}...$. 
* `Thus:` Our greedy algorithm schedules jobs such that the indices are in increasing order: $1,2,3,4...,n$
* `Thus:` If $\sigma^* \neq \sigma$, then there are 2 consecutive jobs $i, j$ such that $i>j$, i.e., i has a higher index than j. Why? Because the greedy schedule is the only schedule where the indices of the jobs will always increase. 
* `Thought experiment` - given our proof thus far, what would happen if we exchange the order of i and j?
* `Cost Benefit Analysis`: How does the completion time of all the jobs change after making this exchange:
  * For other jobs: The completion time of jobs before i and j is unaffected. For jobs coming after i and j, the completion time remains the same. The completion time was the sum of all job lengths up to and including that job. The sum i + j = j + i.
  * For job i, the completion time increases by exactly the length of j
  * For job j, the completion time decreases by exactly the length of i
* `Upshot:` 
  * The cost of the exchange is $w_i \cdot l_j$, since i has to wait for job j to complete. 
  * The benefit of the exchange is $w_j \cdot l_i$, since job j no longer has to wait for job i to complete.
* `Note:` Because of our renaming assumption, $i>j \rightarrow \frac{w_i}{l_i}<\frac{w_j}{l_j} \rightarrow w_il_j < w_jl_i \rightarrow \text{cost of exchange < benefit of exchange}$. 
* `Thus:` The swap improves $\sigma^*$, which contradicts the $\sigma^*$ optimality assumption. 

**Handling Ties**

* `Claim:` GreedyRatio algorithm (ordering jobs in non-increasing order) is correct, even with ties. 
* `New Proof Plan:` 
  * Fix arbitrary input of n jobs. Again, we want to prove the correctness for the general case
  * Let $\sigma=$the greedy schedule and $\sigma^*=$any other optimal schedule
  * Will show that $\sigma$ is at least as good as $\sigma^*$. And since $\sigma^*$ was arbitrary, then the greedy schedule is at least as good as every other schedule, and therefore is optimal.
* `Assume:` We rename jobs such that $\frac{w_1}{l_1}>\frac{w_2}{l_2}>\frac{w_3}{l_3}...$.
* `Consider` an arbitrary schedule $\sigma^*$.
* If $\sigma^*=\sigma$, we're done. No further proof is needed. 
* If $\sigma^*\neq\sigma$, then $\exists$ consecutive jobs $i,j$ such that $i > j$. 
  * `Note:` From our renaming assumption, $i>j \rightarrow \frac{w_i}{l_i} \leq \frac{w_j}{l_j} \rightarrow w_il_j \leq w_jl_i$. Since there are ties, we use the weaker comparison of less than or equal to.
  * `Further note:` Exchanging i and j from $\sigma^*$ has a net benefit of $w_jl_i - w_il_j \geq 0$ since j no longer has to wait for i, while i has to now wait for j. This difference is either positive, if jobs i and j are not tied in their ratio, or equal if they are.
  * `Thus:` By inverting i and j, we don't make $\sigma^*$ worse.
* `Upshot:` 
  * Exchanging **adjacent inversions** like i and j only makes $\sigma^*$ better
  * The exchange decreases the number of **inverted pairs** - this is because when we exchange one such pair, we uninvert the inversion. And since they are adjacent, we don't create new inversions. Thus, the number of inversions decreases by exactly one each time we do such an exchange
* In the previous proof, benefit from an exchange was enough to produce a contradiction, and we were done. In this proof, though, we simply repeatedly exchange inverted pairs until there are none left, ending up with a greedy schedule.
* Why can this not continue forever? There can only be at most ${n}\choose{2}$ inversions. After at most that many inversions, we necessarily terminate with what becomes the greedy schedule. 
* Since exchanging inversions can not make $\sigma^*$ worse, then $\sigma$ is at least as good as $\sigma^*$.
* So greedy is optimal.

#### Problem 2: Minimum Spanning Trees

**Overview**

* **Informal Goal: **Connect a bunch of points together as cheaply as possible.
* **Applications: **Clustering (more later), networking
* Boasts blazingly fast greedy algorithms:
  * `Prim's algorithm` [1957, Djikstra 1959, Jarnick 1930]
  * `Kruskal's algorithms` [1956]
  * $O(m \log n)$, where m = # of edges, n = # of vertices
  * Using heap data structure (as in Djikstra's), and Union find data structure
  
**Problem Definition**

* **Input:** undirected graphs $G=(V,E)$, cost $c_e$ for each edge $e \in E$
  * Assume adjacency list representation - we have an array of vertices, an array of edges, and pointers connecting vertices to their incident edges and connecting edges back to their two endpoints
  * Ok if edge costs are negative 
* **Output:** minimum cost spanning tree $T \in E$ that spans all vertices. What does this mean?
  * `Cost` is the sum of edge costs in T
  * T should have no cycles, i.e. should be a `tree`
  * The subgraph $(V,T)$ should be connected (should be a path between each pair of vertices), i.e `spans all vertices`
* **Assumption #1:** Input graph G is itself connected
  * If this assumption is violated, then the problem isn't well defined. Since G is not connected, none of its subgraphs can be connected. Thus there is no MST
  * Connectivity is easy to check in a preprocessing step using DFS or BFS
  * If it's not connected, you can define a general problem, the `minimum cost forest` which returns the minimum cost subgraph that spans the most vertices.
* **Assumption #2:** edge costs are distinct
  * Both Prim and Kruskal's algorithms remain correct if there are ties, regardless of how ties are broken
  
**Prim's MST Algorithm**

* Initialize `X=[s]`, for $s\in V$, chosen arbitrarily
* Initialize `T=null`. An invariant is that the tree-so-far, T, will span X.
* While `X != V`
  * let `e=(u,v)` be the cheapest edge of G with $u \in X, v \notin X$ 
  * Add e to T
  * Add v to X
* **While loop** increases the number of spanned vertices as cheaply as possible.

**Correctness of Prim's Algorithm: Plan**

* **Theorem:** Prim's algorithm always outputs an MST
* **Part 1:** Prim's computes a spanning tree $T^*$. We'll use basic properties of graphs and spanning trees. 
* **Part 2:** $T^*$ is an MST. We'll use the `cut property`.

**Correctness of Prim's Algorithm: Part1 **

* **Claim**: Prim’s algorithm produces a spanning tree  
Let's review some graph properties.

* **Cuts**
  * **Definition:** a cut of a graph is a partition of its vertices into 2 non-empty sets. 
  * Assume you have 2 partitions,A & B. What are the characteristics of the edges with reference to these partitions?
    * Edges that have their endpoints in A
    * Those that have their endpoints in B
    * **Cut edges:** Those that cross the cut; they have one endpoint in A, the other in B. We are interested in these edges.
    * For a given cut of the graph, there can be many edges crossing it
    * For a given edge in the graph, there can be many cuts that that edge crosses.
  * For a graph with n vertices, roughly how many cuts does it have? 
    * You can imagine making a binary choice for each vertex in the graph. Should we place it in partition A or B.
    * So with n vertices, we have roughly $2^n$ cuts. It will be exactly $2^{n-1}$ cuts, since the partitions aren't allowed to be empty.

Next, we prove 3 easy facts about cuts, and once we do, we'll be in a position to prove that Prim's algorithm produces a spanning tree. 

* **Empty Cut Lemma:** This lemma gives us a new characterization of the connectedness of a graph. In particular, we use it define when a graph is not connected
  * A graph G is not connected $\iff \exists$ a cut $(A,B)$ with no crossing edges
  * It's an if and only if statement, so we have to prove both directions.
  * **Proof:** $(\Longleftarrow)$ Assume RHS, i.e., assume there is a cut $(A,B)$ such that no edge crosses it. Show that the graph is not connected.
    * Pick $u\in A$ and $v \in B$.
    * Since there is no edge crossing the cut, then there is no path $u-v$ in G. 
    * Therefore, G is not connected. 
  * **Proof:**$(\Longrightarrow)$ Assume LHS - i.e., assume that the graph G is not connected. Show that there exists a cut such that no edge crosses it.
    * Since the graph is not connected, there has to be a pair of vertices u and v such that the path $u-v$ does not exist.
    * Define partition A as $A={\text{vertices reachable from u}}$ - i.e u's connected components.
    * Define partition B as $B={\text{all other vertices}}$ - i.e. all the connected components other than the one that contains u
    * By definition $u \in A$, and by assumption u and v are not reachable from each other, so $v \in B$ - thus the cut $(A,B)$ is a valid cut of G.
    * Notice that there are no edges crossing this cut. Why is this true? Assume there is an edge crossing that cut. Since A by definition is the set of vertices reachable from u, then u has a path to the vertex that crosses the edge, and therefore has a path to partition B. But by definition, B is the set of vertices not reachable from u. So we have a contradiction.

* **Two Easy Facts**
  * **Double-Crossing Lemma:** Suppose the cycle $C \in E$ has an edge crossing the cut $(A,B)$. Then so does some other edge of C. In general, if there is a cycle with an edge crossing the cut, then the cycles can only cross it an even number of times. 
  * **Lonely Cut Corollary:** If $e$ is the only edge crossing a cut $(A.B)$, then it cannot be in a cycle. `Proof:` If it were in a cycle, then some other edge would also cross the cut, therefore it would not be lonely.

**Correctness Of Prim's Algorithm: Part 1**

* **Claim:** Prim's algorithm produces a spanning tree
* The algorithm maintains an invariant that T (edges picked so far) always spans X (vertices picked so far). This can be proven by induction.
* To prove that we end up with a spanning tree, we have to prove 2 things. One is that there are no cycles in the resulting tree. Second is that the tree actually spans all the vertices. 
* 

**I will continue this proof later on. Right now it's more important to know the algorithms instead.**

**Fast Implentation of Prim's Algorithm**

* Let's revisit Prim's algorithms
  * Initialize `X=[s]`, for $s\in V$, chosen arbitrarily
  * Initialize `T=null`. An invariant is that the tree-so-far, T, will span X.
  * While `X != V`
    * let `e=(u,v)` be the cheapest edge of G with $u \in X, v \notin X$ 
    * Add e to T
    * Add v to X
  * **While loop** increases the number of spanned vertices as cheaply as possible.
* What would be the running time of a straightforward implementation?
  * The initialization step is constant time
  * The while loop iterates n-1 times. Thus we have $O(n)$ where $n=$the number of vertices.
  * The work done inside each iteration is esssentially a bruteforce search for the cheapest edge that crosses the X, V-X cut. You can implement this in $O(m)$ time where $m=$ the number of edges.
    * You could have a boolean associated with each edge indicating whether it's in X or not. So when you see an edge, you know whether it crosses the cut or not. 
  * Total $O(mn)$ time
* The speedup for this algorithm is through deploying a suitable data structure
  * What's happening inside the the main loop is that we're repeatedly looking for the cheapest edge, the minimum cost edge. 
  * The whole idea of using heaps is to speed up repeated minimum computations. 
* **Heaps**
  * A heap contains a set of elements, each with a key associated with it. 
  * Heap provides methods to `Insert`, `Extract-min`, and `Delete` elements, all in $O(log n)$ time
* **Natural Idea:** Store edges in the heaps and use edges costs as keys.
  * This can be done and results in a running time of $O(mlogn)$
  * Make sure that the edges stored in the heap are those which cross the frontier. 
* We're gonna look a cleverer method, which does not improve the running time, but it does give better constants. 
* The key point is to instead store vertices in the heap instead of edges
* We maintain 2 invariants, one describing what the heap contains, the other describing the key values of those heap elements
  * **Invariant 1:** elements in the heap = vertices in $V-X$
  * **Invariant 2:** for $v \in V-X$, $key[v]$ = cheapest edge $(u,v)$, where $u \in X$, or +inf if the v has no edge crossing the cut.
* Given these invariants, can we need to think/check 3 things
  1. Can we initialize the heap such that the invariants are satisfied at the beginning?
  2. Can we simulate Prim's algorithm in each iteration of the while loop?
  3. Can we maintain the invariants after each iteration?
* **How do we set up the heap at the pre-processing step such that the invariants are satisfied?**
  * At the start, $X=\{s\}$, $V-X = $ all other vertices aside from X, and the key values for each vertex $v \in V-X$ is just the cost of cheapest edge to $s$ if there is one or +inf if not.
  * We can initialize this heap in $O(m+n\text{ }log\text{ }n) = O(m\text{ }log\text{ }n)$ preprocessing where an edge scan to compute the key values of vertices to be inserted can be done in $O(m)$ time while the actual inserts of said vertices takes $O(n\text{ }log\text{ }n)$ time.
  * Why $O(m\text{ }log\text{ }n)$? First, we are interested in connected graphs, otherwise talk of spanning trees doesn't make sense. Second, in a connected graph, $m \geq n-1$ always holds. In other words, m is always, asymptotically, at least as big as n. 
* **How do we faithfully simulate each iteration of the while loop in Prim's algorithm?**
  * Since we set up our keys to be the cheapest edge incident to vertices in X, Extract-Min gives us the cheapest edge overall that crosses the cut. So the element returned by extract-min is the vertex we add to X, and its key is gives us the cheapest edge that we then add to T.
* **How do we maintain the invariants after each iteration?**
  * The issue is that we would need to recompute some keys after each iteration. 
  * Once a new vertex v is extracted and added to X, the edge that was crossing the cut is now sucked into X, while (potentially) we now have new edges incident to v that are in $V-X$. These edges are what we need to take care of
  * **Pseudocode:** When v is added to X:  
  for each edge $(v,w) \in E$:  
    if $w \in V-X$: `only edges incident to w will cross the cut`  
      delete w  
      recompute key[w]; key[w] = min(key[w], cost(v,w))  
      reinsert w into heap  
  * A note on the delete operation: In a heap, it's usually delete from a certain position in the heap. 
    * The solution here is to do some additional bookkeeping to keep track of which vertex is at which position in the heap. 

**Runtime analysis of the above**

* Dominated by heap operations
* $n-1$ inserts during preprocessing
* $n-1$  iterations of the while loop
* Each edge in G only triggers an insertion-delete combo only once. Specifically, an edge $(u,v)$ triggers those operations only when the first of those edges is extracted and added to X.(We only care about vertices in V-X). 
* So we have $O(m)$ heap operations, where the heap operations take $O(log\text{ }n)$ time.
* Thus, we have overall running time of $O(m\text{ }log\text{ }n)$
* It's one of the **Four Free Primitives**


**Kruskal's Algorithm**

* **MST Review**
  * **Input:** an undirected graph $G=(V,E)$, edge costs $c_e$
  * **Output:** minimum cost spanning tree (i.e, no cycles, connected)
  * **Assumptions:** G is connected, edge costs are distinct
  * **Cut Property:** If $e$ is the cheapest edge that crosses some cut $(A,B)$, then $e$ is in the MST.
* The idea in Kruskal's algorithm is to pick the next cheapest edge, with the constraint that it does not create a cycle. 
* **Pseudocode:**
* Sort edges in increasing order of edge costs. Rename the edges so that they conform to this sorted order. That is, $[1,2,3...m]$ so that $c_1<c_2<c_3...c_m$
* Initialize our tree-so-far - $T = null$
* for $i=1\text{ to }m$:
  * If $T \cup {e_i}$ does not form a cycle:
    * Add $e_i$ to T

**Straightforward Implementation of the Algorithm**

* **Running time:**
  * Sorting the edges - $O(m\text{ }log\text{ }n)$- Why not $m\text{ }log\text{ }m$??
    * Because log m and log n are interchangeable inside the Big-O notation.
    * The number of edges m is at most quadratic the number of vertices, i.e at most $O(n^2)$. Therefore $O(log\text{ }m)$ is at most $O(2log\text{ }n)$, 2 is just a constant.
  * Initializing T - $O(1)$
  * The for loop iterates m times - $O(m)$
    * Adding $e_i$ to T is constant time. Checking that $e_i$ doesn't form a cycle though, is the issue. That can be done in $O(n)$ time. 
    * Checking if the new edge will add a cycle involves checking whether there exists a path in T between the endpoints of the new edge. If there is, we skip it, if there's not we add it.
    *Use BFS or DFS
  * **Total:** $O(m\text{ }log\text{ }n)+O(mn) = O(mn)$
* Here the cycle check is what's holding us back, since it takes $O(n)$ time to check for a cycle within a single iteration.
* Can we speed that up? Yes - using the **Union-Find data structure**

**The Union-Find Data Structure**

* The raison d'etre for a union-find data structure is to maintain partitions of a set of objects, e.g groups $c_1, c_2, c_3..$ of a set of objects
* We want this data structure to support 2 operations:
  * **FIND(x):** return the name of the group to which x belongs to
  * **UNION($c_i, c_j$):** fuse the two groups $c_i, c_j$ into one
* Why is this useful for Kruskal's algorithms?
  * Think of Kruskal's as working conceptually like this: At the start, when T is empty, all the edges are separate and are therefore their own partitions. Adding an edge to T (as long as it doesn't form a cycle) **fuses** two separate components/partitions into 1.
  * If I want to add edge $(u,v)$, I should first check what component u and v belogn to, so FIND(u) and FIND(v). If both these belong in the same group, then they form a cycle. If they don't, then I add $(u,v)$ and then fuse the 2 components that u and v belong to
  * objects = vertices
  * groups = connected components of the currently chosesn edges in T
  * adding new edge to T corresponds to fusing connected components of u and v

* **Union Find Basics**
* **Idea #1** Maintain one linked structure per connected component - each vertex will have an extra pointer to it ?? **I still don't quite understand this part**
  * Each connected component has an arbitrary leader vertex
  * **Invariant:** Each vertex points to the leader of it's group/component. In effect, each vertex inherits the name of its leader. So we refer to a particular component and its vertices via the name of that component's leader
  * **Key Point:** Given $(u,v)$, we can check if u and v are in the same component in $O(1)$ time. They are in the same component if and only if their leader names in the union-find structure match.
  * **Note:** When $(u,v)$ doesn't create a cycle, we have to add it to T, and thus we have to fuse the connected components of those 2 vertices. In that case, the leader names have to be updated
  * At worst, $O(n)$ leader updates have to be made, which is a bummer. 
* **Idea #2:** When merging two components, have the smaller size component inherit its leader from the larger component.
  * How do we quickly check which component is bigger? You can just augment the objects further to maintain a size field which is updated any time two components merge. (new size = sum of the sizes of components being merged)
  * However, the worst case running time for leader updates is still $O(n)$. If at the end you have 2 components of size $\frac{n}{2}$, then you're still doing $O(n)$ amount of work in merging the two. 
  * **However, however: ** adopting a vertex-centric view, how many times does a single vertex have its leader pointer updated? - $O(log\text{ }n)$
    * **WHY?** Because everytime a vertex v has its leader updated, it's because the other group is at least as big as v's group. So v's group essentially doubles. We started out with components each of size 1, so the doubling can only happen $\leq log_2n$ times.

**Running time for fast implementation**

* $O(m\text{ }log\text{ }n)$ for sorting
* $O(m)$ time for cycle checks - $O(1)$ per iteration
* $O(n\text{ }log\text{ }n)$ for maintaining the union-find invariant. Here we don't bound the leader update operation per iteration, rather we do a global analysis of the total number of updates that a vertex will experience. Each vertex only experiences a maximum of $log_2n$ updates, therefore leader updates for all vertices are bound by $O(n\text{ }log\text{ }n)$
* Total running time is $O(m\text{ }log\text{ }n)$, which is the new bottleneck of this algorithm (the sorting operation)


## Data Structures {#data_structures}

### Introduction

* **What's the point of data structures?** organize data so that it can be accessed quickly and usefully.
* **Examples:** lists, queues, stacks, heaps, search trees, hash tables, bloom filters, union-find 
* Different data structures support different sets of operations which are suitable for different tasks.
* Generally speaking, the fewer the operations that a data structure supports, the faster those operations are and the smaller a space overhead required by the data structure
* **Rule of thumb:** Choose the minimal data structure with all the operations you need for your application, but no more. 
* **Levels to this:**
  * **Level 0:** "What's a data structure?" i.e, no knowledge of data structures
  * **Level 1:** Cocktail party level knowledge; the person is comfortable holding a conversation about them as well as basic operations but is shaky when it comes to implementing them in their application or in a technical interview context.
  * **Level 2:** They're comfortable using them in their own applications and have good sense of which data structure is appropriate for which tasks.
  * **Level 3:** They have an understanding of the underlying implementations of these data structures.

### Heaps: Operations & Applications

#### Supported Operations  
A heap is a container for objects, each with a key associated with it.  
The basic operations are:

* **INSERT(H, x):** Insert a new object x to the heap H
  * Running Time: $O(log\text{ }n)$
* **EXTRACT-MIN(H):** Remove and return the object with the minimum key value from H (or a pointer to it)
  * Running Time: $O(log\text{ }n)$
  * Here **EXTRACT-MAX** would work as well.
* **HEAPIFY:** this operation performs n batched inserts in $O(n)$ time. (you could think of inserting the n objects one by one, which would take $O(n\text{ }log\text{ }n)$ time
* **DELETE:** delete any object, not just the min, from the heap in $O(log\text{ }n)$ time.

#### Applications  
First, consider the **canonical use of a heap:** When you notice that your application is doing repeated minimum computations, especially by exhaustive search. Heaps are a fast way to do these repeated minimum computations.

**Application 1: Sorting**

* The **selection sort** runs in $O(n^2)$ time, and the way we actually sort is that we search for the minimum of the array, swap it out with the first element, then search for the minimum of the remaining unsorted portion, and swap it out etc. 
* The **repeated minimum computations** and the exhaustive search should hint at a heap data structure. Using a heap, we get a sorting algorithm called:
* **Heapsort:**
  1. Insert all elements of the unsorted array into the heap - $O(n log\text{ }n)$ or $O(n)$ depending on which operation you use for inserts.
  2. Repeatedly extract-min as you add them into the result array - $O(log\text{ }n)$
  * **Running time:** we have n number of inserts and extract-mins, so we have $O(n\text{ }log\text{ }n)$ time.
  * Heap sort is a **comparison-based** sorting algorithm, and therefore is optimal (no comparison based algorithm can do better than $O(log\text{ }n)$)

**Application 2: Median Maintenance**

* **Input:** a sequence of $x_1, X_2...x_n$ one by one.
* The task is to output at each step the median of the sequence thus far
* **Constraint:** use $O(log\text{ }i)$ where i is the # of numbers passed thus far
* **Solution:** maintain 2 heaps; $\text{Heap}_{LOW}$ and $\text{Heap}_{HIGH}$ supporting extraxt-max and extract-min respectively. 
* **Key idea:** maintain the invariant that that the lowest half of the elements so far are in $\text{Heap}_{LOW}$ and the highest half are in $\text{Heap}_{HIGH}$ 
* **2 realizations:**
  * You gotta check that you can maintain this invariant in $O(log\text{ }i)$ time. 
  * Given the invariant, check that you can compute the median in $O(log\text{ }i)$ time as well.
  
#### Python implementation

**Abstract Data Structure vs Concrete Data Structure**

* The python **heapq** module implements a min priority queue.
* **Note:** the `heap` is a concrete data structure while the `heapq` is an abstract data structure.
  * An **abstract data structure** determines the interface - it specifies the operations that that data structure needs to support as well as the relationship between them. 
  * A **concrete data structure** implements the operations of an ADS and also provide performance guarantees.
* In general, the largest caveats of an ADS are less meaningful than the bottlenecks that plague its concrete implementation.

**Implementation of Heaps as Complete Binary Trees**

* These are implemented using **complete binary trees**
  * Each node aside from the leaves has 2 child nodes, and each level except possibly the last one must be filled. If the last level isn't filled, then we have nodes as far to the left as possible.
* The **completeness** property ensures that the height of the tree = $log_2n$ rounded up, where n is the number of elements.
* A priority queue maintains the **heap property**
  * The value of a node is always smaller than its children

**Heaps as Lists in the Python heapq module**  
Heaps are implemented as `lists` in the heapq module.   
The relationship between elements are described by 3 rules. For the element at index k in the list

  * first child is at index $2\cdot k+1$
  * second child is at index $2\cdot k + 2$
  * parent is at index $k//2$
  
Keep in mind that an element might not have children. We can run the condition below for each element k (or at most half the list) to check whether the heap maintains the heap property. It should never evaluate to `False` 

  *`h[k] <= h[2*k + 1] and h[k] <= h[2*k + 2]`  

* **Basic Operations**
  * The heapq module doesn't implement a class, but rather it has functions that operate on lists directly.
  * The `heapify` function turns a list into a heap
      ```python
      import heapq
      a = [3,5,1,3,4,1,5]
    import heapq
    a = [3, 5, 1, 2, 6, 8, 7]
    heapq.heapify(a)
    a
    [1, 2, 3, 5, 6, 8, 7]
    ```
  * The `heappop` function removes and returns the highest priority element, while maintaining the heap property.
    ```python
    heapq.heappop(a)
    1
    ```
  * The `heappush` element inserts an element into the heap, maintaining the heap property
    ```
    heapq.heappush(a,0)
    a
    [0,2,3,5,6,8,7]
    ```
  * Heap elements can be tuples as well - used in cases where the element itself does not determine its priority.
  ```python
  heapq.heappush(a, (1, some_object))
  heapq.heappush(b, (2, some_other_object))
  ```
  * The heapq module also defines 2 more operations
    * `heapreplace()` = `heappop()` followed by `heappush`
    * `heappushpop()` = `heappush()` followed by `heappop`

* **A High-level Operation**
  * Heaps are often used to merge 2 sorted sequences, therefore python has an implementation of merge as well. 
  * Merge assumes takes as input 2 iterables and assumes that they are sorted. It outputs an **iterator**, not a list
    ``` python
    heapq.merge(list_a, list_b)
    ```
    
**Problems heaps can solve**

* We've looked at (have we?) merging sorted sequences, merging log files, scheduling periodic tasks.
* Heaps can also be used to get the top n or bottom n elements. The heapq module has high level functions for this.
  * `heapq.nsmallest(n, iterable, key)` 
  * `heapq.nlargest(n, iterable, key)`

**How to identify problems**

* As mentioned earlier, the raison d'etre for heaps is repeatedly computing minimum computations quickly.
* It's a good tool for solving problems involving extremes, like the largest or smallest of a given metric
* 

### Monotonic Queues
* A `monotonic queue` is a queue such that the elements in it are either monotonically increasing or decreasing. The idea is that if you repeatedly pop items from a monotonic queue, the extracted elements should form a sequence that is:
  * monotonically increasing/decreasing
  * includes the last item added
* **Monotonic Increasing Queue** - forms a monotonically increasing sequence when popping its elements
* **Monotonic Decreasing Queue** - forms a monotonically decreasing sequence when popping its elements
* To push an element `e` to a monotic increasing queue, we start from the rear and pop all the elements `s` such that `s >= e`.
* In the case of a monotonic decreasing queue, we start from the rear, and pop all the elements `s` where `s<=e`
* A monotonic queue should support the `add` and `delete` operations, from both the left and right sides.
* 

## Python {#python}
* Initializing a 2d list in this way: `l = [[None]*3]*3` will result in only one instance of the inner list, with 3 references to it. This is utterly interesting.
 
## RShiny {#rshiny}
### Inputs & Outputs
#### Inputs
textInput()  
sliderInput()  
selectInput()  
numericalInput()  
dateRangeInput()  
all inputs have the format
```{r echo=FALSE, eval=FALSE}
library(shiny)
library(dplyr)
library(tidyr)
```
``` {r eval=FALSE}
__input("inputId",
        "label",
        unique_param1, unique_param2, ...)
```
#### Render Functions
Render functions are used to built outputs in the server based on
inputs and possibly other stuff
```{r eval=FALSE}
renderText()
renderTable()
renderImage()
renderPlot()
```
#### Output Functions
Output functions are used in the ui to display the result built by 
render functions in the server
```{r eval=FALSE}
textOutput()
plotOutput()
tableOutput() or dataTableOutput()
imageOutput()
```
##### Non-shiny output and render functions
DT, leaflet, and plotly -> interactive data tables, maps, and plots as Shiny outputs  
In order to add an output to a Shiny app, we need to:  
1. Create the output -> could be a plot, table, string, etc
2. Render the output in the `server` function using appropriate `Render___` function.
3. Assign this render to a variable name prefixed with `output$___`
3. Use the corresponding `___Output` and pass in the variable name
### Layouts and Themes
#### Layouts
1. Sidebar layouts
```{r eval = FALSE}
sidebarLayout(
  sidebarPanel(insert input/output here),
  mainPanel(insert input/output here)
)
```
2. Tab layouts
```{r eval=FALSE}
sidebarLayout(
  sidebarPanel(insert input/output here),
  mainPanel(
    
    tabsetPanel(
      tabPanel(),
      tabPanel()
    )
  )
)
```
#### Themes
Add a theme selector to your Ui
```{r eval=FALSE}
shinythemes::themeSelector()
```
Then you can add it to your U  
### Building The App: A Process  
1. Add inputs to the `ui()`
2. Add outputs to the `ui()`/`server()`
3. Modify the app layout in the `ui()`
4. Update the output in the `server()` to incorporate the input  
## References {#references}
